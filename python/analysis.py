# Mam 10 funkcji do zapenienia wynikami z pliku csv
# Funkcje maj by zdefiniowane w pliku analysis.py
# Funkcje maj by wywoywane z pliku main.py
# Funkcje maj by wywoywane z main.py
# Funkcje maj by wywoywane z main.py
import pandas as pd
import os



# Sownik wymaga na podstawie Twojej tabeli
NORMS = {
    'M1': {'Lav': 2.0,  'Uo': 0.4,  'Ul': 0.7,  'TI': 10, 'Rei': 0.35},
    'M2': {'Lav': 1.5,  'Uo': 0.4,  'Ul': 0.7,  'TI': 10, 'Rei': 0.35},
    'M3': {'Lav': 1.0,  'Uo': 0.4,  'Ul': 0.6,  'TI': 15, 'Rei': 0.30},
    'M4': {'Lav': 0.75, 'Uo': 0.4,  'Ul': 0.6,  'TI': 15, 'Rei': 0.30},
    'M5': {'Lav': 0.5,  'Uo': 0.35, 'Ul': 0.4,  'TI': 15, 'Rei': 0.30},
    'M6': {'Lav': 0.3,  'Uo': 0.35, 'Ul': 0.4,  'TI': 20, 'Rei': 0.30},
}


# ----------------------------  FOLDER SELECTION ------------------------------- #
class AnalysisCalculator:
    def __init__(self):
        self.csv_files = []
        self.cache_dir = "data_cache"

        # DEFINICJA TWOICH KOLUMN (atrybut klasy dostpny wszdzie)
        self.chosen_columns = [
            'Ldc name', 'Lamp info', 'Total flux [lm]', 'Total power [W]',
            'Power/km  [W/km]', 'Road W[m]', 'Lum pos y [m]', 'Lph [m]',
            'Delta [m]', 'Tilt [掳]', 'Lav [cd/m2]', 'Uo (L)', 'Ul', 'TI [%]', 'Rei'
        ]

        # Tworzenie folderu na pliki binarne, jeli nie istnieje
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)

    def set_csv_files(self, csv_files):
        """Otrzymuje list cie偶ek do plik贸w z GUI"""
        self.csv_files = csv_files

    @staticmethod
    def label_norms_vectorized(df):
        """Szybkie sprawdzanie norm dla caego chunka (miliony wierszy w sekund)"""
        df['best_class'] = "Brak"
        # Sprawdzamy od M6 do M1, aby najwy偶sza speniona klasa bya ostateczn
        for class_name in ['M6', 'M5', 'M4', 'M3', 'M2', 'M1']:
            n = NORMS[class_name]
            mask = (
                    (df['Lav [cd/m2]'] >= n['Lav']) &
                    (df['Uo (L)'] >= n['Uo']) &
                    (df['Ul'] >= n['Ul']) &
                    (df['TI [%]'] <= n['TI']) &
                    (df['Rei'] >= n['Rei'])
            )
            df.loc[mask, 'best_class'] = class_name

        # SZYBKI TEST:
        print("\n--- PODGLD PO ETYKIETOWANIU ---")
        print(df[['Ldc name', 'best_class']].head())
        print("Rozkad klas w tej partii:")
        print(df['best_class'].value_counts())
        return df

    def convert_all_to_parquet(self):
        """G贸wna ptla mielca 30GB CSV -> Parquet"""
        if not self.csv_files:
            print("Bd: Nie wybrano 偶adnych plik贸w!")
            return

        for csv_file in self.csv_files:
            base_name = os.path.basename(csv_file).replace(".csv", "")
            # Tworzymy podfolder dla ka偶dego pliku, bo Parquet najlepiej trzyma w czciach
            file_cache_path = os.path.join(self.cache_dir, base_name)

            if os.path.exists(file_cache_path):
                print(f"Pominito: {base_name} (folder cache ju偶 istnieje)")
                continue

            os.makedirs(file_cache_path)
            print(f"\n--- START KONWERSJI: {base_name}.csv ---")

            try:
                # Czytanie w kawakach po 100 000 wierszy
                chunks = pd.read_csv(
                    csv_file,
                    usecols=self.chosen_columns,
                    chunksize=100000,
                    encoding='cp1250',
                    sep=None,
                    engine='python'
                )

                for i, chunk in enumerate(chunks):
                    # 1. Oznaczamy normy
                    chunk = self.label_norms_vectorized(chunk)

                    # 2. Zapisujemy chunk jako oddzielny plik parquet w folderze
                    chunk_filename = f"part_{i}.parquet"
                    chunk.to_parquet(
                        os.path.join(file_cache_path, chunk_filename),
                        engine='pyarrow',
                        index=False
                    )

                    if i % 10 == 0:
                        print(f"Przetworzono {i * 100000} wierszy...")

                print(f"Zakoczono sukcesem: {base_name}")

            except Exception as e:
                print(f"Bd krytyczny przy pliku {csv_file}: {e}")

    def get_results_for_class(self, target_class="M3"):
        """Pobiera i agreguje dane dla konkretnej klasy owietleniowej"""
        import glob

        all_dfs = []
        # Szukamy wszystkich podfolder贸w w cache (dla r贸偶nych ukad贸w/plik贸w)
        folders = glob.glob(os.path.join(self.cache_dir, "*"))

        for folder in folders:
            if os.path.isdir(folder):
                # Wczytujemy dane (Pandas automatycznie poczy wszystkie part_X.parquet)
                temp_df = pd.read_parquet(folder)

                # Pobieramy nazw ukadu z nazwy folderu (np. 'wycinek_100k_rows')
                layout_name = os.path.basename(folder)
                temp_df['Source_Layout'] = layout_name
                all_dfs.append(temp_df)

        if not all_dfs:
            return "Brak danych w cache. Uruchom najpierw konwersj."

        df = pd.concat(all_dfs, ignore_index=True)

        # 1. Filtrujemy wiersze, kt贸re speniaj klas target_class
        # (Wybieramy t klas LUB wy偶sze, np. dla M3 bierzemy te偶 M2 i M1)
        classes = ['M6', 'M5', 'M4', 'M3', 'M2', 'M1']
        valid_classes = classes[classes.index(target_class):]

        filtered = df[df['best_class'].isin(valid_classes)].copy()

        if filtered.empty:
            return f"呕adna oprawa nie spenia normy {target_class}."

        # 2. RACJONALIZACJA: Dla ka偶dej oprawy szukamy wiersza z najni偶szym 'Power/km  [W/km]'
        # Grupowanie po oprawie i lampie
        best_results = (
            filtered.sort_values(by='Power/km  [W/km]', ascending=True)
            .drop_duplicates(subset=['Ldc name', 'Lamp info'])
        )

        # Zwracamy Top 10 najbardziej 'racjonalnych' rozwiza
        return best_results[[
            'Ldc name', 'Lamp info', 'Total power [W]',
            'Power/km  [W/km]', 'Delta [m]', 'best_class', 'Source_Layout'
        ]].head(10)

    def load_full_data(self, file_name_no_ext):
        """Przykad wczytywania danych dla jednego ukadu (np. do wykresu)"""
        path = os.path.join(self.cache_dir, file_name_no_ext)
        if os.path.exists(path):
            # Pandas wczyta wszystkie pliki part_X.parquet z folderu jako jedn tabel!
            return pd.read_parquet(path)
        return None

    def calculate_results(self):
        """Metoda uruchamiana przyciskiem Start w GUI"""
        print("Rozpoczynam proces przygotowania danych...")
        self.convert_all_to_parquet()
        print("\nWSZYSTKIE DANE S GOTOWE W FORMACIE BINARNYM.")


